{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e11c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 04:21:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"first\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133a3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "import findspark\n",
    "findspark.init('/usr/local/spark')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "conf = pyspark.SparkConf().setMaster(\"local\").setAppName(\"first\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b868cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f38fbe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql import Row\n",
    "users = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"first_name\": \"Corrie\",\n",
    "        \"last_name\": \"Van den Oord\",\n",
    "        \"email\": \"cvandenoord0@etsy.com\",\n",
    "        \"gender\": \"male\",\n",
    "        \"current_city\": \"Dallas\",\n",
    "        \"phone_numbers\": Row(mobile=\"+1 234 567 8901\", home=\"+1 234 567 8911\"),\n",
    "        \"courses\": [1, 2],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 1000.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 15),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 10, 1, 15, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"first_name\": \"Nikolaus\",\n",
    "        \"last_name\": \"Brewitt\",\n",
    "        \"email\": \"nbrewitt1@dailymail.co.uk\",\n",
    "        \"gender\": \"male\",\n",
    "        \"current_city\": \"Houston\",\n",
    "        \"phone_numbers\":  Row(mobile=\"+1 234 567 8923\", home=\"1 234 567 8934\"),\n",
    "        \"courses\": [3],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 900.0,\n",
    "        \"customer_from\": datetime.date(2021, 2, 14),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 2, 18, 3, 33, 0)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"first_name\": \"Orelie\",\n",
    "        \"last_name\": \"Penney\",\n",
    "        \"email\": \"openney2@vistaprint.com\",\n",
    "        \"gender\": \"female\",\n",
    "        \"current_city\": \"\",\n",
    "        \"phone_numbers\": Row(mobile=\"+1 714 512 9752\", home=\"+1 714 512 6601\"),\n",
    "        \"courses\": [2, 4],\n",
    "        \"is_customer\": True,\n",
    "        \"amount_paid\": 850.55,\n",
    "        \"customer_from\": datetime.date(2021, 1, 21),\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 3, 15, 15, 16, 55)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"first_name\": \"Ashby\",\n",
    "        \"last_name\": \"Maddocks\",\n",
    "        \"email\": \"amaddocks3@home.pl\",\n",
    "        \"gender\": \"male\",\n",
    "        \"current_city\": \"San Fransisco\",\n",
    "        \"phone_numbers\": Row(mobile=None, home=None),\n",
    "        \"courses\": [],\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 10, 17, 45, 30)\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"first_name\": \"Kurt\",\n",
    "        \"last_name\": \"Rome\",\n",
    "        \"email\": \"krome4@shutterfly.com\",\n",
    "        \"gender\": \"female\",\n",
    "        \"current_city\": None,\n",
    "        \"phone_numbers\": Row(mobile=\"+1 817 934 7142\", home=None),\n",
    "        \"courses\": [],\n",
    "        \"is_customer\": False,\n",
    "        \"amount_paid\": None,\n",
    "        \"customer_from\": None,\n",
    "        \"last_updated_ts\": datetime.datetime(2021, 4, 2, 0, 55, 18)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2edf3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "144cf3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-------------+--------------------+----------+------+---+-----------+------------+-------------------+--------------------+\n",
      "|amount_paid|courses| current_city|customer_from|               email|first_name|gender| id|is_customer|   last_name|    last_updated_ts|       phone_numbers|\n",
      "+-----------+-------+-------------+-------------+--------------------+----------+------+---+-----------+------------+-------------------+--------------------+\n",
      "|    1000.55| [1, 2]|       Dallas|   2021-01-15|cvandenoord0@etsy...|    Corrie|  male|  1|       true|Van den Oord|2021-02-10 01:15:00|{+1 234 567 8901,...|\n",
      "|      900.0|    [3]|      Houston|   2021-02-14|nbrewitt1@dailyma...|  Nikolaus|  male|  2|       true|     Brewitt|2021-02-18 03:33:00|{+1 234 567 8923,...|\n",
      "|     850.55| [2, 4]|             |   2021-01-21|openney2@vistapri...|    Orelie|female|  3|       true|      Penney|2021-03-15 15:16:55|{+1 714 512 9752,...|\n",
      "|       null|     []|San Fransisco|         null|  amaddocks3@home.pl|     Ashby|  male|  4|      false|    Maddocks|2021-04-10 17:45:30|        {null, null}|\n",
      "|       null|     []|         null|         null|krome4@shutterfly...|      Kurt|female|  5|      false|        Rome|2021-04-02 00:55:18|{+1 817 934 7142,...|\n",
      "+-----------+-------+-------------+-------------+--------------------+----------+------+---+-----------+------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15d6eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id| current_city|\n",
      "+---+-------------+\n",
      "|  1|       Dallas|\n",
      "|  2|      Houston|\n",
      "|  3|             |\n",
      "|  4|San Fransisco|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(\"id\",\"current_city\").filter(col(\"current_city\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb956d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id| current_city|\n",
      "+---+-------------+\n",
      "|  1|       Dallas|\n",
      "|  2|      Houston|\n",
      "|  3|             |\n",
      "|  4|San Fransisco|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    ".select(\"id\",\"current_city\")\\\n",
    ".where('current_city IS NOT NULL')\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c9d7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|customer_from|\n",
      "+---+-------------+\n",
      "|  4|         null|\n",
      "|  5|         null|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"customer_from\").where(\"customer_from IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a915d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|customer_from|\n",
      "+---+-------------+\n",
      "|  4|         null|\n",
      "|  5|         null|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"customer_from\").where(col(\"customer_from\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2881e868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  4|San Fransisco|         null|\n",
      "|  5|         null|         null|\n",
      "|  1|       Dallas|   2021-01-15|\n",
      "|  3|             |   2021-01-21|\n",
      "|  2|      Houston|   2021-02-14|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"current_city\",\"customer_from\").orderBy(\"customer_from\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9c5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  5|         null|         null|\n",
      "|  3|             |   2021-01-21|\n",
      "|  1|       Dallas|   2021-01-15|\n",
      "|  2|      Houston|   2021-02-14|\n",
      "|  4|San Fransisco|         null|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"current_city\",\"customer_from\").orderBy(\"current_city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d075c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  2|      Houston|   2021-02-14|\n",
      "|  3|             |   2021-01-21|\n",
      "|  1|       Dallas|   2021-01-15|\n",
      "|  4|San Fransisco|         null|\n",
      "|  5|         null|         null|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('id','current_city','customer_from').orderBy(df.customer_from.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc7f7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  2|      Houston|   2021-02-14|\n",
      "|  3|             |   2021-01-21|\n",
      "|  1|       Dallas|   2021-01-15|\n",
      "|  4|San Fransisco|         null|\n",
      "|  5|         null|         null|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"current_city\",\"customer_from\").orderBy(desc_nulls_last(\"customer_from\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdd5c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7bdfbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.option(\"header\",True).csv(\"zipcode.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b998380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|      null|\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ed56185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06a39e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+-----------------+-----+----------+\n",
      "| id|zipcode|  type|             city|state|population|\n",
      "+---+-------+------+-----------------+-----+----------+\n",
      "|  4|  76166|UNIQUE|CINGULAR WIRELESS|   TX|     84000|\n",
      "+---+-------+------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.dropna().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4768e993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  5|  76177|STANDARD|               null|   TX|      null|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.drop('all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d809be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-----------------+-----+----------+\n",
      "| id|zipcode|    type|             city|state|population|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "|  3|    709|    null|     BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|CINGULAR WIRELESS|   TX|     84000|\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.drop(subset='population').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55a00864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-----------------+-----+----------+\n",
      "| id|zipcode|    type|             city|state|population|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "|  4|  76166|  UNIQUE|CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|             null|   TX|      null|\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.dropna(\"all\",subset=\"type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31baff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  5|  76177|STANDARD|               null|   TX|      null|\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.fill(30000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2859ebc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|               null|   PR|     30100|\n",
      "|  5|  76177|STANDARD|               null|   TX|      null|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.fillna(30000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0706e6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|             Mumbai|   PR|     30100|\n",
      "|  5|  76177|STANDARD|             Mumbai|   TX|      null|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  2|    704|    null|PASEO COSTA DEL SUR|   PR|      null|\n",
      "|  3|    709|    null|       BDA SAN LUIS|   PR|      3700|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.fillna(\"Mumbai\",subset=\"city\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd12c1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-------------------+-----+----------+\n",
      "| id|zipcode|    type|               city|state|population|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "|  1|    704|STANDARD|             Mumbai|   PR|     30100|\n",
      "|  5|  76177|STANDARD|             Mumbai|   TX|     30000|\n",
      "|  4|  76166|  UNIQUE|  CINGULAR WIRELESS|   TX|     84000|\n",
      "|  2|    704|     VIP|PASEO COSTA DEL SUR|   PR|     30000|\n",
      "|  3|    709|     VIP|       BDA SAN LUIS|   PR|      3700|\n",
      "+---+-------+--------+-------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.fill({'type':\"VIP\",'population':30000, 'city':\"Mumbai\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ee2ef3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+-----------------+-----+----------+\n",
      "| id|zipcode|    type|             city|state|population|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "|  1|    704|STANDARD|             null|   PR|     30100|\n",
      "|  4|  76166|  UNIQUE|CINGULAR WIRELESS|   TX|     84000|\n",
      "|  3|    709|    null|     BDA SAN LUIS|   PR|      3700|\n",
      "+---+-------+--------+-----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.na.drop(thresh=5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06e6346d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  1|       Dallas|   2021-01-15|\n",
      "|  2|      Houston|   2021-02-14|\n",
      "|  3|           LA|   2021-01-21|\n",
      "|  4|San Fransisco|         null|\n",
      "|  5|         null|         null|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"current_city\",\"customer_from\").replace(\"\",\"LA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c7e3065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-------------+\n",
      "| id| current_city|customer_from|\n",
      "+---+-------------+-------------+\n",
      "|  1|           LA|   2021-01-15|\n",
      "|  2|      Houston|   2021-02-14|\n",
      "|  3|             |   2021-01-21|\n",
      "|  4|San Fransisco|         null|\n",
      "|  5|         null|         null|\n",
      "+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\",\"current_city\",\"customer_from\").replace(\"Dallas\",\"LA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7e00a9",
   "metadata": {},
   "source": [
    "# Baby Names Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c455d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferschema\", True).csv(\"Baby_Names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2d2551e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------+---+-----+\n",
      "|Year|First Name| County|Sex|Count|\n",
      "+----+----------+-------+---+-----+\n",
      "|2007|      ZOEY|  KINGS|  F|   11|\n",
      "|2007|      ZOEY|SUFFOLK|  F|    6|\n",
      "|2007|      ZOEY| MONROE|  F|    6|\n",
      "|2007|      ZOEY|   ERIE|  F|    9|\n",
      "|2007|       ZOE| ULSTER|  F|    5|\n",
      "+----+----------+-------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd6a6fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2007| 6367|\n",
      "|2013| 6158|\n",
      "|2014| 8362|\n",
      "|2012| 6164|\n",
      "|2009| 6312|\n",
      "|2010| 6192|\n",
      "|2011| 6216|\n",
      "|2008| 6481|\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Year\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7e0c8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52252"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0796df80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|2014| 8362|\n",
      "|2013| 6158|\n",
      "|2012| 6164|\n",
      "|2011| 6216|\n",
      "|2010| 6192|\n",
      "|2009| 6312|\n",
      "|2008| 6481|\n",
      "|2007| 6367|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Year\").count().sort(col(\"Year\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "971c41fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"processed_data/Baby_Names_Parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3f5233f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").option(\"path\",\"processed_data/Baby_Names_Table\").saveAsTable(\"babynames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2f339ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o358.saveAsTable.\n: java.io.FileNotFoundException: File file:/home/labuser/Desktop/Shyam/spark-warehouse/processed_data/Baby_Partition does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\tat org.apache.spark.sql.execution.command.RepairTableCommand.scanPartitions(ddl.scala:751)\n\tat org.apache.spark.sql.execution.command.RepairTableCommand.run(ddl.scala:703)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:201)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_data/Baby_Partition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbabynamesyear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1521\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msaveAsTable(name)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o358.saveAsTable.\n: java.io.FileNotFoundException: File file:/home/labuser/Desktop/Shyam/spark-warehouse/processed_data/Baby_Partition does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\n\tat org.apache.spark.sql.execution.command.RepairTableCommand.scanPartitions(ddl.scala:751)\n\tat org.apache.spark.sql.execution.command.RepairTableCommand.run(ddl.scala:703)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:190)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:189)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:201)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:675)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:570)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").partitionBy(\"Year\").option(\"path\",\"processed_data/Baby_Partition\").saveAsTable(\"babynamesyear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14a48e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-----+----+\n",
      "|First Name|County|Sex|Count|Year|\n",
      "+----------+------+---+-----+----+\n",
      "+----------+------+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from babynamesyear\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d1e65a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".partitionBy(\"Year\")\\\n",
    ".option(\"path\",\"/home/labuser/Desktop/Shyam/processed_data/Baby_Names_Year\")\\\n",
    ".saveAsTable(\"babynames_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bffec9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 82:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write\\\n",
    ".mode(\"overwrite\")\\\n",
    "# .option(\"maxRecordsPerFile\",4000)\n",
    ".partitionBy(\"Year\",\"Sex\")\\\n",
    ".option(\"path\",\"/home/labuser/Desktop/Shyam/processed_data/Baby_Names_Year_Gender/\")\\\n",
    ".saveAsTable(\"babynames_year_gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29e477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
